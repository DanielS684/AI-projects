{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skill_creator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjKOBJRRDXDm"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWvg6E4CJeEK"
      },
      "source": [
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YxGuWPc5Kuf",
        "outputId": "4f8d0226-89b8-4282-cc04-97f49eaba46b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import gym\n",
        "import pybulletgym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "def Identity(x):\n",
        "    return x\n",
        "\n",
        "def weights_init_(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "class Normalizer():\n",
        "    def __init__(self, nb_inputs):\n",
        "        self.n = torch.zeros(nb_inputs)\n",
        "        self.mean = torch.zeros(nb_inputs)\n",
        "        self.mean_diff = torch.zeros(nb_inputs)\n",
        "        self.var = torch.zeros(nb_inputs)\n",
        "\n",
        "    def observe(self, x):\n",
        "        self.n += 1.0\n",
        "        last_mean = self.mean.clone().detach()\n",
        "        self.mean += (x - self.mean) / self.n\n",
        "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
        "        self.var = (self.mean_diff / self.n).clamp(min = 1e-2)\n",
        "\n",
        "    def normalize(self, inputs):\n",
        "        obs_mean = self.mean\n",
        "        obs_std = torch.sqrt(self.var)\n",
        "        return (inputs - obs_mean) / obs_std\n",
        "    \n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, hidden_depth, output_size=None, use_output_layer=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_output_layer = use_output_layer\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        in_size = input_size\n",
        "        for _ in range(hidden_depth):\n",
        "            fc = nn.Linear(in_size, hidden_size)\n",
        "            in_size = hidden_size\n",
        "            self.hidden_layers.append(fc)\n",
        "\n",
        "        if use_output_layer:\n",
        "            self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "        else:\n",
        "            self.output_layer = Identity\n",
        "        \n",
        "        self.apply(weights_init_)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = F.relu(hidden_layer(x))\n",
        "\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "LOG_SIG_MAX = 2\n",
        "LOG_SIG_MIN = -20\n",
        "epsilon = 1e-6\n",
        "\n",
        "class GaussianPolicy(nn.Module):\n",
        "    def __init__(self, num_inputs, hidden_size, hidden_depth, num_actions, action_space):\n",
        "        super(GaussianPolicy, self).__init__()\n",
        "        \n",
        "        self.net = MLP(num_inputs, hidden_size, hidden_depth, use_output_layer=False)\n",
        "\n",
        "        self.mean_linear = nn.Linear(hidden_size, num_actions)\n",
        "        self.log_std_linear = nn.Linear(hidden_size, num_actions)\n",
        "\n",
        "        self.apply(weights_init_)\n",
        "\n",
        "        if action_space is None:\n",
        "            self.action_scale = torch.tensor(1.)\n",
        "            self.action_bias = torch.tensor(0.)\n",
        "        else:\n",
        "            self.action_scale = torch.FloatTensor(\n",
        "                (action_space.high - action_space.low) / 2.)\n",
        "            self.action_bias = torch.FloatTensor(\n",
        "                (action_space.high + action_space.low) / 2.)\n",
        "\n",
        "    def sample(self, x):\n",
        "        x = F.relu(self.net(x))\n",
        "\n",
        "        mean = self.mean_linear(x)\n",
        "        log_std = self.log_std_linear(x)\n",
        "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
        "\n",
        "        return mean, log_std\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean, log_std = self.sample(x)\n",
        "        std = log_std.exp()\n",
        "        normal = Normal(mean, std)\n",
        "      \n",
        "        x_t = normal.rsample()\n",
        "        y_t = torch.tanh(x_t)\n",
        "\n",
        "        action = y_t * self.action_scale + self.action_bias\n",
        "\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + epsilon)\n",
        "        log_prob = log_prob.sum(-1, keepdim=True)\n",
        "\n",
        "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
        "\n",
        "        return mean, action, log_prob.squeeze()\n",
        "\n",
        "class SAC():\n",
        "    def __init__(self,\n",
        "                obs_size,\n",
        "                action_size,\n",
        "                hidden_size,\n",
        "                hidden_depth,\n",
        "                action_space=None,\n",
        "                tau=0.005,\n",
        "                alpha=0.2,\n",
        "                gamma=0.99,\n",
        "                policy_lr=3e-4,\n",
        "                critic_lr=3e-4,\n",
        "                predict_lr=3e-4,\n",
        "                batch_size=64\n",
        "                ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tau = tau\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.predict = MLP(obs_size+obs_size, hidden_size, hidden_depth, 1)\n",
        "\n",
        "        self.policy = GaussianPolicy(obs_size+1, hidden_size, hidden_depth, action_size, action_space)\n",
        "\n",
        "        self.critic1 = MLP(obs_size+action_size+1, hidden_size, hidden_depth, 1)\n",
        "        self.critic2 = MLP(obs_size+action_size+1, hidden_size, hidden_depth, 1)\n",
        "\n",
        "        self.critic1_target = MLP(obs_size+action_size+1, hidden_size, hidden_depth, 1)\n",
        "        self.critic2_target = MLP(obs_size+action_size+1, hidden_size, hidden_depth, 1)\n",
        "\n",
        "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
        "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
        "\n",
        "        self.critic_parameters = list(self.critic1.parameters()) + list(self.critic2.parameters())\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=policy_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic_parameters, lr=critic_lr)\n",
        "        self.predict_optimizer = optim.Adam(self.predict.parameters(), lr=predict_lr)\n",
        "\n",
        "        self.norm = Normalizer(obs_size)\n",
        "\n",
        "        self.replay_buffer = []\n",
        "\n",
        "        self.target_entropy = -np.prod((action_size,)).item()\n",
        "        self.log_alpha = torch.zeros(1, requires_grad=True)\n",
        "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=policy_lr)\n",
        "\n",
        "    def cat(self, list):\n",
        "        return torch.cat(list, dim=-1)\n",
        "\n",
        "    def store(self, state, skill, action, next_state, next_skill, done):\n",
        "        self.replay_buffer.append({\"state\": state,\n",
        "                                   \"skill\": skill,\n",
        "                                   \"action\": torch.tensor(action),\n",
        "                                   \"next_state\": next_state,\n",
        "                                   \"next_skill\": next_skill,\n",
        "                                   \"done\": torch.tensor(int(not done))})\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        rand_batch = random.choices(self.replay_buffer, k=batch_size)\n",
        "\n",
        "        batch = {\"states\": [],\n",
        "                 \"skills\": [],\n",
        "                 \"actions\": [],\n",
        "                 \"next_states\": [],\n",
        "                 \"next_skills\": [],\n",
        "                 \"dones\": []}\n",
        "\n",
        "        for dict in rand_batch:\n",
        "            batch[\"states\"].append(dict[\"state\"])\n",
        "            batch[\"skills\"].append(dict[\"skill\"])\n",
        "            batch[\"actions\"].append(dict[\"action\"])\n",
        "            batch[\"next_states\"].append(dict[\"next_state\"])\n",
        "            batch[\"next_skills\"].append(dict[\"next_skill\"])\n",
        "            batch[\"dones\"].append(dict[\"done\"])\n",
        "\n",
        "        batch = {key: torch.stack(value_list, dim=0) for key, value_list in batch.items()}\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def skill_predict(self, states, skills, next_states, timestep):\n",
        "        pred_skills = torch.tanh(self.predict(self.cat([states, next_states])))\n",
        "\n",
        "        skill_loss = F.mse_loss(pred_skills, skills, reduction='none')\n",
        "\n",
        "        rewards = torch.tanh(-torch.log(skill_loss.detach()))\n",
        "\n",
        "        predict_loss = skill_loss.mean()\n",
        "\n",
        "        writer.add_scalar(\"Avg Rewards\", rewards.mean(), timestep)\n",
        "\n",
        "        self.predict_optimizer.zero_grad()\n",
        "        predict_loss.backward()\n",
        "        self.predict_optimizer.step()\n",
        "\n",
        "        return rewards\n",
        "\n",
        "\n",
        "    def train(self, timestep):\n",
        "        batch = self.sample(self.batch_size)\n",
        "\n",
        "        states = batch['states']\n",
        "        skills = batch['skills']\n",
        "        actions = batch['actions']\n",
        "        next_states = batch['next_states']\n",
        "        next_skills = batch['next_skills']\n",
        "        dones = batch['dones']\n",
        "\n",
        "        states = self.norm.normalize(states)\n",
        "        next_states = self.norm.normalize(next_states)\n",
        "\n",
        "        rewards = self.skill_predict(states, skills, next_states, timestep)\n",
        "\n",
        "        _, pi, log_pi = self.policy(self.cat([states, skills]))\n",
        "        _, next_pi, next_log_pi = self.policy(self.cat([next_states, next_skills]))\n",
        "        q1 = self.critic1(self.cat([states, actions, skills])).squeeze(1)\n",
        "        q2 = self.critic2(self.cat([states, actions, skills])).squeeze(1)\n",
        "\n",
        "        min_q_pi = torch.min(self.critic1(self.cat([states, pi, skills])), self.critic2(self.cat([states, pi, skills]))).squeeze(1)\n",
        "        min_q_next_pi = torch.min(self.critic1_target(self.cat([next_states, next_pi, next_skills])),\n",
        "                                  self.critic2_target(self.cat([next_states, next_pi, next_skills]))).squeeze(1)\n",
        "  \n",
        "\n",
        "        v_backup = min_q_next_pi - self.alpha*next_log_pi\n",
        "        q_backup = rewards.squeeze() + self.gamma*dones*v_backup\n",
        "\n",
        "        policy_loss = (self.alpha*log_pi - min_q_pi).mean()\n",
        "        critic1_loss = F.mse_loss(q1, q_backup.detach())\n",
        "        critic2_loss = F.mse_loss(q2, q_backup.detach())\n",
        "        critic_loss = critic1_loss + critic2_loss\n",
        "\n",
        "        writer.add_scalar(\"Policy loss\", policy_loss.item(), timestep)\n",
        "        writer.add_scalar(\"Critic loss\", critic_loss.item(), timestep)\n",
        "\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()    \n",
        "\n",
        "        for critic1_param, critic1_target_param in zip(self.critic1.parameters(), self.critic1_target.parameters()):\n",
        "            critic1_target_param.data.copy_(self.tau*critic1_param.data + (1.0-self.tau)*critic1_target_param.data)\n",
        "\n",
        "        for critic2_param, critic2_target_param in zip(self.critic2.parameters(), self.critic2_target.parameters()):\n",
        "            critic2_target_param.data.copy_(self.tau*critic2_param.data + (1.0-self.tau)*critic2_target_param.data)    \n",
        "\n",
        "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
        "        self.alpha_optimizer.zero_grad()\n",
        "        alpha_loss.backward()\n",
        "        self.alpha_optimizer.step()\n",
        "\n",
        "        self.alpha = self.log_alpha.exp()\n",
        "\n",
        "        writer.add_scalar(\"Alpha\", self.alpha, timestep)\n",
        "\n",
        "env = gym.make(\"AntPyBulletEnv-v0\")\n",
        "observation = env.reset()\n",
        "\n",
        "agent = SAC(env.observation_space.shape[0],\n",
        "            env.action_space.shape[0],\n",
        "            128,\n",
        "            2,\n",
        "            env.action_space)\n",
        "\n",
        "episode_skill = torch.empty(1).uniform_(-1,1)\n",
        "print(episode_skill)\n",
        "train = 5000\n",
        "explore = 5000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WalkerBase::__init__\n",
            "tensor([0.5701])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IJs426NTNx7"
      },
      "source": [
        "for timestep in range(1000001, 1005001):\n",
        "    state = torch.tensor(observation, dtype=torch.float32)\n",
        "\n",
        "    agent.norm.observe(state)\n",
        "\n",
        "    if timestep > explore:\n",
        "        with torch.no_grad():\n",
        "            obs = agent.norm.normalize(state)\n",
        "            _, action, _ = agent.policy(torch.cat([obs, episode_skill], dim=-1))\n",
        "            action = action.squeeze(0).numpy()\n",
        "\n",
        "    else:\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "    observation, _, done, _ = env.step(action)\n",
        "\n",
        "    next_state = torch.tensor(observation, dtype=torch.float32)\n",
        "\n",
        "    agent.store(state, episode_skill, action, next_state, episode_skill, done)\n",
        "\n",
        "    episode_skill = torch.empty(1).uniform_(-1,1)\n",
        "\n",
        "    if done:\n",
        "        agent.norm.observe(state)\n",
        "        observation = env.reset()\n",
        "    \n",
        "\n",
        "    if timestep > train:\n",
        "        agent.train(timestep)\n",
        "        torch.save({\"policy\": agent.policy.state_dict(),\n",
        "                    \"critic1\": agent.critic1.state_dict(),\n",
        "                    \"critic2\": agent.critic2.state_dict(),\n",
        "                    \"critic1_target\": agent.critic1_target.state_dict(),\n",
        "                    \"critic2_target\": agent.critic2_target.state_dict(),\n",
        "                    \"predictor\": agent.predict.state_dict(),\n",
        "                    \"log_alpha\": agent.log_alpha,\n",
        "                    \"alpha\" : agent.alpha,\n",
        "                    \"policy_optim\": agent.policy_optimizer.state_dict(),\n",
        "                    \"critic_optim\": agent.critic_optimizer.state_dict(),\n",
        "                    \"predictor_optim\": agent.predict_optimizer.state_dict(),\n",
        "                    \"alpha_optim\" : agent.alpha_optimizer.state_dict(),\n",
        "                    \"n\": agent.norm.n,\n",
        "                    \"mean\": agent.norm.mean,\n",
        "                    \"mean_diff\": agent.norm.mean_diff,\n",
        "                    \"var\": agent.norm.var}, \"drive/My Drive/models.tar\")\n",
        "    writer.flush()\n",
        "\n",
        "env.close()\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}