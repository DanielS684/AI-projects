{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minecraft.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPhreIVPofcu"
      },
      "source": [
        "!java -version\n",
        "!sudo apt-get purge openjdk-*\n",
        "!java -version\n",
        "!sudo apt-get install openjdk-8-jdk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TB1LCHnoi4H"
      },
      "source": [
        "!pip3 install --upgrade minerl\n",
        "!sudo apt-get install xvfb xserver-xephyr vnc4server\n",
        "!sudo pip install pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83uKe4vy2AAH",
        "cellView": "form"
      },
      "source": [
        "#@title Change python code { output-height: 10 , form-width: 10}\n",
        "%%writefile /usr/local/lib/python3.6/dist-packages/minerl/herobraine/env_specs/navigate_specs.py\n",
        "import sys\n",
        "from typing import List\n",
        "\n",
        "import minerl.herobraine\n",
        "import minerl.herobraine.hero.handlers as handlers\n",
        "from minerl.herobraine.env_specs.simple_env_spec import SimpleEnvSpec\n",
        "\n",
        "\n",
        "class Navigate(SimpleEnvSpec):\n",
        "    def __init__(self, dense, extreme):\n",
        "        suffix = 'Extreme' if extreme else ''\n",
        "        suffix += 'Dense' if dense else ''\n",
        "        name = 'MineRLNavigate{}-v0'.format(suffix)\n",
        "        xml = 'navigation{}.xml'.format(suffix)\n",
        "        self.dense, self.extreme = dense, extreme\n",
        "        super().__init__(name, xml)\n",
        "\n",
        "    def is_from_folder(self, folder: str) -> bool:\n",
        "        return folder == 'navigateextreme' if self.extreme else folder == 'navigate'\n",
        "\n",
        "    def create_mission_handlers(self) -> List[minerl.herobraine.hero.AgentHandler]:\n",
        "        mission_handlers = [\n",
        "            handlers.RewardForTouchingBlock(\n",
        "                {\"diamond_block\", 100.0}\n",
        "            ),\n",
        "            handlers.NavigateTargetReward(),\n",
        "            handlers.NavigationDecorator(\n",
        "                min_radius=64,\n",
        "                max_radius=64,\n",
        "                randomize_compass_target=True\n",
        "            )\n",
        "        ]\n",
        "        if self.dense:\n",
        "            mission_handlers.append(handlers.RewardForWalkingTwardsTarget(\n",
        "                reward_per_block=1, reward_schedule=\"PER_TICK\"\n",
        "            ))\n",
        "        return mission_handlers\n",
        "\n",
        "    def determine_success_from_rewards(self, rewards: list) -> bool:\n",
        "        reward_threshold = 100.0\n",
        "        if self.dense:\n",
        "            reward_threshold += 60\n",
        "        return sum(rewards) >= reward_threshold\n",
        "\n",
        "    def create_observables(self) -> List[minerl.herobraine.hero.AgentHandler]:\n",
        "        return super().create_observables() + [\n",
        "            handlers.CompassObservation(),\n",
        "            handlers.DeathObservation(),\n",
        "            handlers.FlatInventoryObservation(['dirt'])]\n",
        "\n",
        "    def create_actionables(self) -> List[minerl.herobraine.hero.AgentHandler]:\n",
        "        return super().create_actionables() + [handlers.PlaceBlock(['none', 'dirt'])]\n",
        "\n",
        "    def get_docstring(self):\n",
        "        return make_navigate_text(\n",
        "            top=\"normal\" if not self.extreme else \"extreme\",\n",
        "            dense=self.dense)\n",
        "\n",
        "\n",
        "def make_navigate_text(top, dense):\n",
        "    navigate_text = \"\"\"\n",
        ".. image:: ../assets/navigate{}1.mp4.gif\n",
        "    :scale: 100 %\n",
        "    :alt: \n",
        ".. image:: ../assets/navigate{}2.mp4.gif\n",
        "    :scale: 100 %\n",
        "    :alt: \n",
        ".. image:: ../assets/navigate{}3.mp4.gif\n",
        "    :scale: 100 %\n",
        "    :alt: \n",
        ".. image:: ../assets/navigate{}4.mp4.gif\n",
        "    :scale: 100 %\n",
        "    :alt: \n",
        "In this task, the agent must move to a goal location denoted by a diamond block. This represents a basic primitive used in many tasks throughout Minecraft. In addition to standard observations, the agent has access to a “compass” observation, which points near the goal location, 64 meters from the start location. The goal has a small random horizontal offset from the compass location and may be slightly below surface level. On the goal location is a unique block, so the agent must find the final goal by searching based on local visual features.\n",
        "The agent is given a sparse reward (+100 upon reaching the goal, at which point the episode terminates). \"\"\"\n",
        "    if dense:\n",
        "        navigate_text += \"**This variant of the environment is dense reward-shaped where the agent is given a reward every tick for how much closer (or negative reward for farther) the agent gets to the target.**\\n\"\n",
        "    else:\n",
        "        navigate_text += \"**This variant of the environment is sparse.**\\n\"\n",
        "\n",
        "    if top is \"normal\":\n",
        "        navigate_text += \"\\nIn this environment, the agent spawns on a random survival map.\\n\"\n",
        "        navigate_text = navigate_text.format(*[\"\" for _ in range(4)])\n",
        "    else:\n",
        "        navigate_text += \"\\nIn this environment, the agent spawns in an extreme hills biome.\\n\"\n",
        "        navigate_text = navigate_text.format(*[\"extreme\" for _ in range(4)])\n",
        "    return navigate_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w75KRtcoomxm"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(640, 480))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bznqpNxknMuD"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p94OTpdbneG3"
      },
      "source": [
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aW4qZB76j5I"
      },
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "import minerl\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEgMTCkDmhwi"
      },
      "source": [
        "class ContinualAgent(nn.Module):\n",
        "    def __init__(self, context_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        action_size = 64\n",
        "\n",
        "        self.action_limit = 1.0499999523162842\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.bfc1 = nn.Bilinear(1024, context_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, context_size)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        x = F.relu(self.bfc1(x, c))\n",
        "        action = torch.tanh(self.fc2(x))\n",
        "        c = torch.tanh(self.fc3(x))\n",
        "\n",
        "        action = action * self.action_limit\n",
        "\n",
        "        return action, c\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, conv=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = conv\n",
        "\n",
        "        if self.conv:\n",
        "            self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "            self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "            self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "            self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "            self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "            self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "            input_size = 1024\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        if not self.conv:\n",
        "            self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.conv:\n",
        "            x = self.pool(F.relu(self.conv1(x)))\n",
        "            x = self.pool(F.relu(self.conv2(x)))\n",
        "            x = self.pool(F.relu(self.conv3(x)))\n",
        "            x = self.pool(F.relu(self.conv4(x)))\n",
        "            x = self.pool(F.relu(self.conv5(x)))\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        if not self.conv:\n",
        "            x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzPBpQuhmqCC"
      },
      "source": [
        "class Trainer():\n",
        "    def __init__(self, context_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.policy = ContinualAgent(context_size, hidden_size)\n",
        "        self.context_critic = MLP(context_size, hidden_size, 1)\n",
        "        self.state_critic = MLP(None, hidden_size, 1, conv=True)\n",
        "        self.past_predict = MLP(None, hidden_size, context_size, conv=True)\n",
        "        self.future_predict = MLP(None, hidden_size, context_size, conv=True)\n",
        "\n",
        "        self.policy.to(\"cuda:0\")\n",
        "        self.context_critic.to(\"cuda:0\")\n",
        "        self.state_critic.to(\"cuda:0\")\n",
        "        self.past_predict.to(\"cuda:0\")\n",
        "        self.future_predict.to(\"cuda:0\")\n",
        "\n",
        "        self.policy_optimizer = optim.SGD(self.policy.parameters(), lr=0.001)\n",
        "        self.context_critic_optimizer = optim.SGD(self.context_critic.parameters(), lr=0.001)\n",
        "        self.state_critic_optimizer = optim.SGD(self.state_critic.parameters(), lr=0.001)\n",
        "        self.past_predict_optimizer = optim.SGD(self.policy.parameters(), lr=0.001)\n",
        "        self.future_predict_optimizer = optim.SGD(self.policy.parameters(), lr=0.001)\n",
        "\n",
        "        self.mem = []\n",
        "\n",
        "    def train(self, c_action, cur_state, actor_action, c_next_state, next_state, done, total_timestep):\n",
        "\n",
        "        c_state, prev_state = self.mem\n",
        "\n",
        "        context_value = self.context_critic(c_state)\n",
        "\n",
        "        state_value = self.state_critic(cur_state)\n",
        "\n",
        "        next_context_value = self.context_critic(c_next_state)\n",
        "\n",
        "        next_state_value = self.state_critic(next_state)\n",
        "\n",
        "        pred_c_past = torch.tanh(self.past_predict(prev_state))\n",
        "        pred_c_future = torch.tanh(self.future_predict(next_state))\n",
        "\n",
        "        past_history_loss = F.mse_loss(pred_c_past, c_action.detach())\n",
        "        future_history_loss = F.mse_loss(pred_c_future, c_action.detach())\n",
        "\n",
        "        reward = torch.tanh(-torch.log(past_history_loss.detach()) + torch.log(future_history_loss.detach()))\n",
        "\n",
        "        writer.add_scalar(\"Reward\", reward.item(), total_timestep)\n",
        "        writer.add_scalar(\"Prediction: Past Loss\", past_history_loss.item(), total_timestep)\n",
        "        writer.add_scalar(\"Prediction: Future Loss\", future_history_loss.item(), total_timestep)\n",
        "\n",
        "        actor_q_value = reward + 0.99*int(not done)*next_state_value\n",
        "\n",
        "        actor_advantage = actor_q_value - state_value\n",
        "\n",
        "        policy_loss = (-actor_action*actor_advantage.detach()).mean()\n",
        "        state_critic_loss = F.mse_loss(state_value, actor_q_value.detach())\n",
        "\n",
        "        writer.add_scalar(\"Policy: Loss\", policy_loss.item(), total_timestep)\n",
        "        writer.add_scalar(\"Critic: State Loss\", state_critic_loss.item(), total_timestep)\n",
        "\n",
        "        context_q_value = reward + 0.99*int(not done)*next_context_value\n",
        "\n",
        "        context_advantage = context_q_value - context_value\n",
        "\n",
        "        context_loss = (-c_action*context_advantage.detach()).mean()\n",
        "        context_critic_loss = F.mse_loss(context_value, context_q_value.detach())\n",
        "\n",
        "        writer.add_scalar(\"Context: Loss\", context_loss.item(), total_timestep)\n",
        "        writer.add_scalar(\"Critic: Context Loss\", context_critic_loss.item(), total_timestep)\n",
        "\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        policy_loss.backward(retain_graph=True)\n",
        "        context_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "\n",
        "        self.state_critic_optimizer.zero_grad()\n",
        "        self.context_critic_optimizer.zero_grad()\n",
        "        state_critic_loss.backward()\n",
        "        context_critic_loss.backward()\n",
        "        self.context_critic_optimizer.step()\n",
        "        self.state_critic_optimizer.step()\n",
        "\n",
        "        self.past_predict_optimizer.zero_grad()\n",
        "        past_history_loss.backward()\n",
        "        self.past_predict_optimizer.step()\n",
        "\n",
        "        self.future_predict_optimizer.zero_grad()\n",
        "        future_history_loss.backward()\n",
        "        self.future_predict_optimizer.step()\n",
        "\n",
        "def preprocess(state):\n",
        "\n",
        "    image_data = cv2.cvtColor(np.float32(state), cv2.COLOR_RGB2GRAY)\n",
        "    image_data = np.reshape(image_data,(64, 64, 1))\n",
        "    image_tensor = image_data.transpose(2, 0, 1)\n",
        "    image_tensor = image_tensor.astype(np.float32)\n",
        "    state = torch.from_numpy(image_tensor)\n",
        "\n",
        "    return state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbTBmt6zmSvz"
      },
      "source": [
        "print(\"Hello\")\n",
        "env = gym.make('MineRLNavigateVectorObf-v0')\n",
        "obs  = env.reset()\n",
        "print(\"Hi\")\n",
        "\n",
        "trainer = Trainer(512, 512)\n",
        "\n",
        "first_c = torch.zeros(1, 512).to(\"cuda:0\")\n",
        "\n",
        "timestep = 0\n",
        "living = True\n",
        "final_death = 15000\n",
        "done = False\n",
        "\n",
        "while living and timestep <= final_death:\n",
        "\n",
        "    if done:\n",
        "      print(\"I died\")\n",
        "      obs = env.reset()\n",
        "\n",
        "    if timestep >= 1:\n",
        "        c_state, prev_state = trainer.mem\n",
        "\n",
        "        cur_state = (preprocess(obs[\"pov\"]).unsqueeze(0)/255).to(\"cuda:0\")\n",
        "\n",
        "        _ , c_action = trainer.policy(prev_state, c_state)\n",
        "\n",
        "        actor_action, c_next_state = trainer.policy(cur_state, c_action.detach())\n",
        "\n",
        "        env_action = dict(vector=actor_action.detach().cpu().numpy())\n",
        "\n",
        "        obs, _ , done, info = env.step(env_action)\n",
        "\n",
        "        if info != {}:\n",
        "            print(info)\n",
        "\n",
        "        next_state = (preprocess(obs[\"pov\"]).unsqueeze(0)/255).to(\"cuda:0\")\n",
        "\n",
        "        trainer.train(c_action, cur_state, actor_action, c_next_state, next_state, done, timestep)\n",
        "\n",
        "        trainer.mem = [c_action.detach(), cur_state]\n",
        "    \n",
        "    else:\n",
        "        cur_state = (preprocess(obs[\"pov\"]).unsqueeze(0)/255).to(\"cuda:0\")\n",
        "\n",
        "        env_action = env.action_space.sample()\n",
        "\n",
        "        print(env_action)\n",
        "\n",
        "        obs, _ , done, info = env.step(env_action)\n",
        "\n",
        "        trainer.mem = [first_c, cur_state]\n",
        "\n",
        "\n",
        "    timestep += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}